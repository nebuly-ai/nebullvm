{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef331be9",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-nebullvm-works\" target=\"_blank\" style=\"text-decoration: none;\"> How Nebullvm Works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#tutorials\" target=\"_blank\" style=\"text-decoration: none;\"> Tutorials </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#benchmarks\" target=\"_blank\" style=\"text-decoration: none;\"> Benchmarks </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#installation\" target=\"_blank\" style=\"text-decoration: none;\"> Installation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> Get Started </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#optimization-examples\" target=\"_blank\" style=\"text-decoration: none;\"> Optimization Examples </a>\n",
    "</center>\n",
    
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Discord </a> |\n",
    "    <a href=\"https://nebuly.ai/\" target=\"_blank\" style=\"text-decoration: none;\"> Website </a> |\n",
    "    <a href=\"https://www.linkedin.com/company/72460022/\" target=\"_blank\" style=\"text-decoration: none;\"> LinkedIn </a> |\n",
    "    <a href=\"https://twitter.com/nebuly_ai\" target=\"_blank\" style=\"text-decoration: none;\"> Twitter </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260653a",
   "metadata": {},
   "source": [
    "# Accelerate Hugging Face GPT2 and BERT with nebullvmÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf3af5",
   "metadata": {},
   "source": [
    "Hi and welcome ðŸ‘‹\n",
    "\n",
    "In this notebook we will discover how in just a few steps you can speed up the response time of deep learning model inference using the open-source library nebullvm.\n",
    "\n",
    "With nebullvm's latest API, you can speed up models up to 10 times without any loss of accuracy (option A), or accelerate them up to 20-30 times by setting a self-defined amount of accuracy/precision that you are willing to trade off to get even lower response time (option B). To accelerate your model, nebullvm takes advantage of various optimization techniques such as deep learning compilers (in both option A and option B), quantization, half accuracy, and so on (option B).\n",
    "\n",
    "Let's jump to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372ab99",
   "metadata": {},
   "source": [
    "We will first optimize a GPT2 transformer by taking advantage of nebullvm API option A (acceleration without loss of accuracy/precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73072506",
   "metadata": {},
   "source": [
    "## GPT2 - Import a pre-trained Hugging Face model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d55115",
   "metadata": {},
   "source": [
    "We chose GPT2 as the pre-trained model that we want to optimize. Let's download both the pre-trained model and the tokenizer from the Hugging Face model hub.\n",
    "\n",
    "We will also select one short and one long text for GPT2 to process, which it will use at a later stage to test the impact of nebullvm as the input tokens change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633cf21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Short text you wish to process\"\n",
    "long_text = \" \".join([text]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa0739",
   "metadata": {},
   "source": [
    "Let's run the prediction 100 times to calculate the average response time of the unoptimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_short_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for GPT2: ({encoded_input['input_ids'].shape[1]} tokens): {vanilla_short_token_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_encoded_input = tokenizer(long_text, return_tensors='pt', truncation=True)\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        new_out = model(**long_encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_long_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for GPT2: ({long_encoded_input['input_ids'].shape[1]} tokens): {vanilla_long_token_time} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17040431",
   "metadata": {},
   "source": [
    "## GPT2 - Speed up inference with nebullvm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddc21d",
   "metadata": {},
   "source": [
    "It's now time of improving a bit the performance in terms of speed. Let's use `nebullvm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d934f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebullvm.api.frontend.huggingface import optimize_huggingface_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76248033",
   "metadata": {},
   "source": [
    "Using nebullvm is very simple and straightforward! Just use the `optimize_huggngface_model` function and provide as input the model, the tokenizer and text example for the model input, the batch size, the maximum input size for each input (excluding the batch size already defined), and a directory in which to save the optimized model.\n",
    "\n",
    "The function also takes as input some information about the context of the model. In this case, for example, we need to specify that the attention values can be 0 or 1 (in the `extra_input_info` dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_huggingface_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    input_texts=[text],\n",
    "    batch_size=1,\n",
    "    max_input_sizes=[tuple(value.size()[1:]) for value in long_encoded_input.values()],\n",
    "    save_dir=\".\",\n",
    "    extra_input_info=[{}, {\"max_value\": 1, \"min_value\": 0}],\n",
    "    use_torch_api=False,\n",
    "    tokenizer_args={\"truncation\": True},\n",
    "    perf_loss_ths=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0a7a1",
   "metadata": {},
   "source": [
    "Let's run the prediction 100 times to calculate the average response time of the unoptimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e83997",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_short_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for GPT2 ({encoded_input['input_ids'].shape[1]} tokens): {optimized_short_token_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94610",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_new_out = optimized_model(**long_encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_long_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for GPT2 ({long_encoded_input['input_ids'].shape[1]} tokens): {optimized_long_token_time} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62279394",
   "metadata": {},
   "source": [
    "## GPT2 - Print the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64eef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter here your username\n",
    "your_username = \"username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following line for installing gputil if you are running on an NVIDIA GPU.\n",
    "#!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpuinfo\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cpu_info = cpuinfo.get_cpu_info()['brand_raw']\n",
    "gpu_info = \"no\"\n",
    "if torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_info = list(gpus)[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52660cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: GPT2 - HuggingFace\n",
    "Tokens: {encoded_input['input_ids'].shape[1]}\n",
    "- Vanilla performance: {round(vanilla_short_token_time, 2)}ms\n",
    "- Optimized performance: {round(optimized_short_token_time, 2)}ms\n",
    "- Speedup: {round(vanilla_short_token_time/optimized_short_token_time, 1)}x\n",
    "Tokens: {long_encoded_input['input_ids'].shape[1]}\n",
    "- Vanilla performance: {round(vanilla_long_token_time, 2)}ms\n",
    "- Optimized performance: {round(optimized_long_token_time, 2)}ms\n",
    "- Speedup: {round(vanilla_long_token_time/optimized_long_token_time, 1)}x\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714504c5",
   "metadata": {},
   "source": [
    "# BERT - Speed up inference with nebullvm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e33c5",
   "metadata": {},
   "source": [
    "Let's see the nebullvm performance on another model. Let's optimize the popular Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Short text you wish to process\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_bert_short = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for BERT: ({inputs['input_ids'].shape[1]} tokens): {vanilla_bert_short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \". \".join([\"Hello, my dog is cute\"]*100)\n",
    "new_inputs = tokenizer(long_text, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        new_outputs = model(**new_inputs)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_bert_long = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for BERT: ({new_inputs['input_ids'].shape[1]} tokens): {vanilla_bert_long} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1335da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_huggingface_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    input_texts=[text],\n",
    "    batch_size=1,\n",
    "    max_input_sizes=[tuple(value.size()[1:]) for value in new_inputs.values()],\n",
    "    save_dir=\".\",\n",
    "    extra_input_info=[{}, {\"max_value\": 1, \"min_value\": 0}, {\"max_value\": 1, \"min_value\": 0}],\n",
    "    use_torch_api=False,\n",
    "    tokenizer_args={\"truncation\": True},\n",
    "    perf_loss_ths=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4410b",
   "metadata": {},
   "source": [
    "Let's now calculate the time required to run a prediction as an average over 100 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb57285",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = optimized_model(**inputs)\n",
    "    times.append(time.time()-st)\n",
    "optimized_bert_short = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for BERT: ({inputs['input_ids'].shape[1]} tokens): {optimized_bert_short} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = optimized_model(**new_inputs)\n",
    "    times.append(time.time()-st)\n",
    "optimized_bert_long = sum(times)/len(times)*1000\n",
    "print(f\"Average response time for BERT: ({new_inputs['input_ids'].shape[1]} tokens): {optimized_bert_long} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e4d5a",
   "metadata": {},
   "source": [
    "## BERT - Print the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: BERT - HuggingFace\n",
    "Tokens: {inputs['input_ids'].shape[1]}\n",
    "- Vanilla performance: {round(vanilla_bert_short, 2)}ms\n",
    "- Optimized performance: {round(optimized_bert_short, 2)}ms\n",
    "- Speedup: {round(vanilla_bert_short/optimized_bert_short, 1)}x\n",
    "Tokens: {new_inputs['input_ids'].shape[1]}\n",
    "- Vanilla performance: {round(vanilla_bert_long, 2)}ms\n",
    "- Optimized performance: {round(optimized_bert_long, 2)}ms\n",
    "- Speedup: {round(vanilla_bert_long/optimized_bert_long, 1)}x\n",
    "\"\"\"\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb234e5e",
   "metadata": {},
   "source": [
    "Great! Was it easy? How are the results? Do you have any comments?\n",
    "Share your optimization results and thoughts with <a href=\"https://discord.gg/RbeQMu886J\" target=\"_blank\"> our community on Discord</a>, where we chat about nebullvm and AI acceleration.\n",
    "\n",
    "Note that the acceleration of nebullvm depends very much on the hardware configuration and your AI model. Given the same input model, nebullvm can accelerate it by 10 times on some machines and perform poorly on others.\n",
    "\n",
    "If you want to learn more about how nebullvm works, look at other tutorials and performance benchmarks, check out the links below or write to us on Discord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d613e4f",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-nebullvm-works\" target=\"_blank\" style=\"text-decoration: none;\"> How Nebullvm Works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#tutorials\" target=\"_blank\" style=\"text-decoration: none;\"> Tutorials </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#benchmarks\" target=\"_blank\" style=\"text-decoration: none;\"> Benchmarks </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#installation\" target=\"_blank\" style=\"text-decoration: none;\"> Installation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> Get Started </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#optimization-examples\" target=\"_blank\" style=\"text-decoration: none;\"> Optimization Examples </a>\n",
    "</center>\n",
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Discord </a> |\n",
    "    <a href=\"https://nebuly.ai/\" target=\"_blank\" style=\"text-decoration: none;\"> Website </a> |\n",
    "    <a href=\"https://www.linkedin.com/company/72460022/\" target=\"_blank\" style=\"text-decoration: none;\"> LinkedIn </a> |\n",
    "    <a href=\"https://twitter.com/nebuly_ai\" target=\"_blank\" style=\"text-decoration: none;\"> Twitter </a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
