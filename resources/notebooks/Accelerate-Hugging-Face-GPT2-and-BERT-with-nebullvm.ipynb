{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df42416",
   "metadata": {},
   "source": [
    "# Accelerate Huggingface transformers inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73072506",
   "metadata": {},
   "source": [
    "## Import a pre-trained Hugginface model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d55115",
   "metadata": {},
   "source": [
    "We selected GPT2 as pretrained model we want to optimize. Let's download from the huggingface model hub both the pre-trained architecture and tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa0739",
   "metadata": {},
   "source": [
    "Let's run the model 100 times for computing the average latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_short_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {vanilla_short_token_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7312fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \" \".join([text]*100)\n",
    "long_encoded_input = tokenizer(long_text, return_tensors='pt', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        new_out = model(**long_encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_long_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {vanilla_long_token_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17040431",
   "metadata": {},
   "source": [
    "## Nebullvm optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddc21d",
   "metadata": {},
   "source": [
    "It's now time of improving a bit the performance in terms of speed. Let's use `nebullvm`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d934f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebullvm.api.frontend.huggingface import optimize_huggingface_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76248033",
   "metadata": {},
   "source": [
    "To use nebullvm is quite easy and straightforward! Just use the `optimize_huggingface_model` function and give as inputs the model, the tokenizer and example of text for the model input, the batch size, the maximum input sizes for each input (excluding the already defined batch size) and a directory where you want to save the optimized model. \n",
    "\n",
    "Furthermore, the function takes as input some context information about the model. In this case for instance we need to specify that attention values can be either 0 or 1 (in the `extra_input_info` dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_huggingface_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    input_texts=[text],\n",
    "    batch_size=1,\n",
    "    max_input_sizes=[tuple(value.size()[1:]) for value in long_encoded_input.values()],\n",
    "    save_dir=\".\",\n",
    "    extra_input_info=[{}, {\"max_value\": 1, \"min_value\": 0}],\n",
    "    use_torch_api=False,\n",
    "    tokenizer_args={\"truncation\": True},\n",
    "    perf_loss_ths=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e83997",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_out = optimized_model(**encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_short_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {optimized_short_token_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94610",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        final_new_out = optimized_model(**long_encoded_input)\n",
    "    times.append(time.time()-st)\n",
    "optimized_long_token_time = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {optimized_long_token_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64eef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put here your username\n",
    "your_username = \"DiegoFiori\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomment the following line for installing gputil (if you are running on an NVIDIA GPU)\n",
    "#!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpuinfo\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cpu_info = cpuinfo.get_cpu_info()['brand_raw']\n",
    "gpu_info = \"no\"\n",
    "if torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_info = list(gpus)[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52660cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: GPT2 - HuggingFace - tokens {encoded_input['input_ids'].shape[1]}\n",
    "Vanilla performance: {round(vanilla_short_token_time, 2)}ms\n",
    "Optimized performance: {round(optimized_short_token_time, 2)}ms\n",
    "Acceleration: {round(vanilla_short_token_time/optimized_short_token_time, 1)}x\n",
    "\"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d151be",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: GPT2 - HuggingFace - tokens {long_encoded_input['input_ids'].shape[1]}\n",
    "Vanilla performance: {round(vanilla_long_token_time, 2)}ms\n",
    "Optimized performance: {round(optimized_long_token_time, 2)}ms\n",
    "Acceleration: {round(vanilla_long_token_time/optimized_long_token_time, 1)}x\n",
    "\"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714504c5",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e33c5",
   "metadata": {},
   "source": [
    "Let's see the nebullvm performance on another model. Let's optimize the popular Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, my dog is cute\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_bert_short = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {vanilla_bert_short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \". \".join([\"Hello, my dog is cute\"]*100)\n",
    "new_inputs = tokenizer(long_text, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        new_outputs = model(**new_inputs)\n",
    "    times.append(time.time()-st)\n",
    "vanilla_bert_long = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {vanilla_bert_long}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1335da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_huggingface_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    input_texts=[text],\n",
    "    batch_size=1,\n",
    "    max_input_sizes=[tuple(value.size()[1:]) for value in new_inputs.values()],\n",
    "    save_dir=\".\",\n",
    "    extra_input_info=[{}, {\"max_value\": 1, \"min_value\": 0}, {\"max_value\": 1, \"min_value\": 0}],\n",
    "    use_torch_api=False,\n",
    "    tokenizer_args={\"truncation\": True},\n",
    "    perf_loss_ths=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb57285",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = optimized_model(**inputs)\n",
    "    times.append(time.time()-st)\n",
    "optimized_bert_short = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {optimized_bert_short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = optimized_model(**new_inputs)\n",
    "    times.append(time.time()-st)\n",
    "optimized_bert_long = sum(times)/len(times)*1000\n",
    "print(f\"Model run 100 times with average latency {optimized_bert_long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e4d5a",
   "metadata": {},
   "source": [
    "Copy - paste the message on the main discussion :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: BERT - HuggingFace - tokens {inputs['input_ids'].shape[1]}\n",
    "Vanilla performance: {round(vanilla_bert_short, 2)}ms\n",
    "Optimized performance: {round(optimized_bert_short, 2)}ms\n",
    "Acceleration: {round(vanilla_bert_short/optimized_bert_short, 1)}x\n",
    "\"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: BERT - HuggingFace - tokens {new_inputs['input_ids'].shape[1]}\n",
    "Vanilla performance: {round(vanilla_bert_long, 2)}ms\n",
    "Optimized performance: {round(optimized_bert_long, 2)}ms\n",
    "Acceleration: {round(vanilla_bert_long/optimized_bert_long, 1)}x\n",
    "\"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
