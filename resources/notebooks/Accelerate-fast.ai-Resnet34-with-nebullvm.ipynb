{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81872ff",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-nebullvm-works\" target=\"_blank\" style=\"text-decoration: none;\"> How Nebullvm Works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#tutorials\" target=\"_blank\" style=\"text-decoration: none;\"> Tutorials </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#benchmarks\" target=\"_blank\" style=\"text-decoration: none;\"> Benchmarks </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#installation\" target=\"_blank\" style=\"text-decoration: none;\"> Installation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> Get Started </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#optimization-examples\" target=\"_blank\" style=\"text-decoration: none;\"> Optimization Examples </a>\n",
    "</center>\n",
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Discord </a> |\n",
    "    <a href=\"https://nebuly.ai/\" target=\"_blank\" style=\"text-decoration: none;\"> Website </a> |\n",
    "    <a href=\"https://www.linkedin.com/company/72460022/\" target=\"_blank\" style=\"text-decoration: none;\"> LinkedIn </a> |\n",
    "    <a href=\"https://twitter.com/nebuly_ai\" target=\"_blank\" style=\"text-decoration: none;\"> Twitter </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc20d2c",
   "metadata": {},
   "source": [
    "# Accelerate fast.ai Resnet34 with nebullvm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e64a20",
   "metadata": {},
   "source": [
    "Hi and welcome ðŸ‘‹\n",
    "\n",
    "In this notebook we will discover how in just a few steps you can speed up the response time of deep learning model inference using the open-source library nebullvm.\n",
    "\n",
    "With nebullvm's latest API, you can speed up models up to 10 times without any loss of accuracy (option A), or accelerate them up to 20-30 times by setting a self-defined amount of accuracy/precision that you are willing to trade off to get even lower response time (option B). To accelerate your model, nebullvm takes advantage of various optimization techniques such as deep learning compilers (in both option A and option B), quantization, half accuracy, and so on (option B).\n",
    "\n",
    "Let's jump to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889aaa2",
   "metadata": {},
   "source": [
    "# Fine-tune a fast.ai model\n",
    "\n",
    "For the tutorial, we will use a fast.ai notebook for beginners in which we will classify whether the input image contains a cat (`True`Â label) or a dog (`False`Â label). Note that this notebook is not intended to be an in-depth guide to fast.ai libraries but, rather, shows how to use nebullvm to speed up fast.ai algorithms at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1402ae",
   "metadata": {},
   "source": [
    "First let's download a sample of images of dogs and cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224), num_workers=0)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e964642",
   "metadata": {},
   "source": [
    "Let's now fine-tune the fast.ai model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6e600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_loss, error = learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70ed97",
   "metadata": {},
   "source": [
    "Now that we have fine-tuned the model, let's calculate the time required to run a prediction as an average over 100 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed1fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    preds = learn.predict(files[0])\n",
    "    times.append((time.time()-st)*1000)\n",
    "fastai_vanilla_time = sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average prediction time: {fastai_vanilla_time} ms,\\nPrediction: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8812918",
   "metadata": {},
   "source": [
    "# Speed up fast.ai inference with nebullvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe42819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nebullvm import optimize_torch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618fcef",
   "metadata": {},
   "source": [
    "Nebullvm was built to be very easy to use. To optimize a model, you only need to specify the model, the batch size and input size for each input tensor, and a directory in which to save the optimized model. In the example, we chose the same directory in which this notebook runs.\n",
    "\n",
    "With the latest API, there are two ways to use nebullvm:\n",
    "\n",
    "- Option A: Accelerate the model up to ~10 times without losing in performances (accuracy/precision/etc.)\n",
    "- Option B: Accelerate the model up to ~30 times with a pre-defined maximum loss in performances\n",
    "    - B1: Performance is measured on your dataset with a performance metric that you specify.\n",
    "    - B2: Performance is measured on your dataset using precision as a metric.\n",
    "    - B3: Performance is measured using precision as a metric.\n",
    "\n",
    "To learn more about how to use nebullvm, check out the <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> readme on GitHub </a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f0dfe",
   "metadata": {},
   "source": [
    "In this example, we provide the code to run option B.1 on the fast.ai model and you find commented out code for the other options as well. \n",
    "\n",
    "We set:\n",
    "\n",
    "- Accuracy as performance metric\n",
    "- Performance loss threshold to around 0.001\n",
    "\n",
    "Now we prepare the dataset so that it can be processed by nebullvm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aacd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for i, (x, y) in enumerate(dls.train):\n",
    "    if i >=10:\n",
    "        break\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "xs = torch.cat(xs, dim=0)\n",
    "ys = torch.cat(ys, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_nebullvm = [((x.unsqueeze(dim=0),), y.unsqueeze(0)) for x, y in zip(xs, ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc7029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Option A: 2-10x acceleration, NO performance loss\n",
    "\n",
    "# optimized_model = optimize_torch_model(\n",
    "#     model=original_model,\n",
    "#     batch_size=1,\n",
    "#     input_sizes=[(3, 224, 224)],\n",
    "#     save_dir=\".\",\n",
    "#     #use_torch_api=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbf726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Option B.1: 2-30x acceleration, maximum performance loss below your \n",
    "## perf_loss_ths, where performance is measured on your dataset dl_nebullvm\n",
    "## with your performance metric perf_metric.\n",
    "\n",
    "optimized_model = optimize_torch_model(\n",
    "    model=original_model,\n",
    "    batch_size=1,\n",
    "    save_dir=\".\",\n",
    "    dataloader=dl_nebullvm,\n",
    "    #use_torch_api=True,\n",
    "    perf_loss_ths=0.005,\n",
    "    perf_metric=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1991b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option B.2: 2-30x acceleration, maximum performance loss below your \n",
    "## perf_loss_ths, where performance is measured on your dataset dl_nebullvm\n",
    "## using precision as a metric. Read more on GitHub on how to set perf_loss_ths.\n",
    "\n",
    "# optimized_model = optimize_torch_model(\n",
    "#     model=original_model,\n",
    "#     batch_size=1,\n",
    "#     save_dir=\".\",\n",
    "#     dataloader=dl_nebullvm,\n",
    "#     #use_torch_api=True,\n",
    "#     perf_loss_ths=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, core):\n",
    "        super().__init__()\n",
    "        self.core = optimized_model\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        res = self.core(*args, **kwargs)\n",
    "        if isinstance(res, tuple) and len(res) == 1:\n",
    "            res = res[0]\n",
    "        return res\n",
    "    \n",
    "    def parameters(self, *args, **kwargs):\n",
    "        yield torch.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_model = ModelWrapper(optimized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = core_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.valid.bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f05743",
   "metadata": {},
   "outputs": [],
   "source": [
    "optz_valid_loss, optz_error = learn.validate(dl=learn.dls.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463ce2e",
   "metadata": {},
   "source": [
    "Let's calculate the time required to run a prediction as an average over 100 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fde6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    preds = learn.predict(files[0])\n",
    "    times.append((time.time()-st)*1000)\n",
    "optimized_time = sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average prediction time: {optimized_time} ms,\\nAverage prediction: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e412875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Full precision error: {error}\\nError after optimization: {optz_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abbc14",
   "metadata": {},
   "source": [
    "## Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter here your username\n",
    "your_username = \"username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following line for installing gputil if you are running on an NVIDIA GPU.\n",
    "#!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpuinfo\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cpu_info = cpuinfo.get_cpu_info()['brand_raw']\n",
    "gpu_info = \"no\"\n",
    "if torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_info = list(gpus)[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I tested nebullvm with the following setup.\n",
    "\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: {learn.arch.__name__} - fast.ai for image classification\n",
    "Vanilla performance: {round(fastai_vanilla_time, 2)}ms\n",
    "Optimized performance: {round(optimized_time, 2)}ms\n",
    "Speedup: {round(fastai_vanilla_time/optimized_time, 1)}x\n",
    "With error increase of {round((optz_error-error)/error*100, 1)}%\n",
    "\"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9415e",
   "metadata": {},
   "source": [
    "Great! Was it easy? How are the results? Do you have any comments?\n",
    "Share your optimization results and thoughts with <a href=\"https://discord.gg/RbeQMu886J\" target=\"_blank\"> our community on Discord</a>, where we chat about nebullvm and AI acceleration.\n",
    "\n",
    "Note that the acceleration of nebullvm depends very much on the hardware configuration and your AI model. Given the same input model, nebullvm can accelerate it by 10 times on some machines and perform poorly on others.\n",
    "\n",
    "If you want to learn more about how nebullvm works, look at other tutorials and performance benchmarks, check out the links below or write to us on Discord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1d05c",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-nebullvm-works\" target=\"_blank\" style=\"text-decoration: none;\"> How Nebullvm Works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#tutorials\" target=\"_blank\" style=\"text-decoration: none;\"> Tutorials </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#benchmarks\" target=\"_blank\" style=\"text-decoration: none;\"> Benchmarks </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#installation\" target=\"_blank\" style=\"text-decoration: none;\"> Installation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> Get Started </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#optimization-examples\" target=\"_blank\" style=\"text-decoration: none;\"> Optimization Examples </a>\n",
    "</center>\n",
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Discord </a> |\n",
    "    <a href=\"https://nebuly.ai/\" target=\"_blank\" style=\"text-decoration: none;\"> Website </a> |\n",
    "    <a href=\"https://www.linkedin.com/company/72460022/\" target=\"_blank\" style=\"text-decoration: none;\"> LinkedIn </a> |\n",
    "    <a href=\"https://twitter.com/nebuly_ai\" target=\"_blank\" style=\"text-decoration: none;\"> Twitter </a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
