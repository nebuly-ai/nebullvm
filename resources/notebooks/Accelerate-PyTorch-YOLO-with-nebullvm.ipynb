{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c977e4a",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-nebullvm-works\" target=\"_blank\" style=\"text-decoration: none;\"> How Nebullvm Works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#tutorials\" target=\"_blank\" style=\"text-decoration: none;\"> Tutorials </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#benchmarks\" target=\"_blank\" style=\"text-decoration: none;\"> Benchmarks </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#installation\" target=\"_blank\" style=\"text-decoration: none;\"> Installation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> Get Started </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#optimization-examples\" target=\"_blank\" style=\"text-decoration: none;\"> Optimization Examples </a>\n",
    "</center>\n",
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Discord </a> |\n",
    "    <a href=\"https://nebuly.ai/\" target=\"_blank\" style=\"text-decoration: none;\"> Website </a> |\n",
    "    <a href=\"https://www.linkedin.com/company/72460022/\" target=\"_blank\" style=\"text-decoration: none;\"> LinkedIn </a> |\n",
    "    <a href=\"https://twitter.com/nebuly_ai\" target=\"_blank\" style=\"text-decoration: none;\"> Twitter </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240f0ea",
   "metadata": {},
   "source": [
    "# Accelerate PyTorch YOLO with nebullvm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfcd562",
   "metadata": {},
   "source": [
    "Hi and welcome ðŸ‘‹\n",
    "\n",
    "In this notebook we will discover how in just a few steps you can speed up the response time of deep learning model inference using the open-source library nebullvm.\n",
    "\n",
    "With nebullvm's latest API, you can speed up models up to 10 times without any loss of accuracy (option A), or accelerate them up to 20-30 times by setting a self-defined amount of accuracy/precision that you are willing to trade off to get even lower response time (option B). To accelerate your model, nebullvm takes advantage of various optimization techniques such as deep learning compilers (in both option A and option B), quantization, half accuracy, and so on (option B).\n",
    "\n",
    "Let's jump to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f5afa",
   "metadata": {},
   "source": [
    "# Install and test YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d727d",
   "metadata": {},
   "source": [
    "Let's install YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f6a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f49833",
   "metadata": {},
   "source": [
    "We start by downloading the model from the Torch hub and running an initial inference on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc46f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import types\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, force_reload=True)\n",
    "\n",
    "# Images\n",
    "imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14415d7f",
   "metadata": {},
   "source": [
    "Let's now calculate the time required to run a prediction as an average over 100 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    starting_time = time.time()\n",
    "    # Inference\n",
    "    results = model(imgs)\n",
    "    times.append((time.time()-starting_time)*1000)\n",
    "yolo_vanilla_time = sum(times) / len(times)\n",
    "print(f\"Average prediction time: {yolo_vanilla_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea18697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.print()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19ef7e",
   "metadata": {},
   "source": [
    "Here we are! We got a good prediction, but it took a while :) Let's see if we are able to speed up the model a little bit without losing in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d07ab0",
   "metadata": {},
   "source": [
    "## Optimization with nebullvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebullvm import optimize_torch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a42b9",
   "metadata": {},
   "source": [
    "First, we need to slightly modify YOLO's forward method. \n",
    "\n",
    "The last layer of the YOLOv5 implementation can create problems on certain hardware for some deep learning compilers that run on the nebullvm core. Since nebullvm aims to be hardware agnostic, we circumvent any potential obstacles by splitting the network body from the head (the last layer) and optimizing only the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfaf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_model = copy.deepcopy(model.model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_once(self, x, profile=False, visualize=False):\n",
    "    y, dt = [], []  # outputs\n",
    "    for m in self.model:\n",
    "        if m.f != -1:  # if not from previous layer\n",
    "            x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n",
    "        if profile:\n",
    "            self._profile_one_layer(m, x, dt)\n",
    "        x = m(x)  # run\n",
    "        y.append(x if m.i in self.save else None)  # save output\n",
    "        if visualize:\n",
    "            feature_visualization(x, m.type, m.i, save_dir=visualize)\n",
    "    self.last_y = y\n",
    "    return x\n",
    "core_model._forward_once = types.MethodType(_forward_once, core_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197ca91",
   "metadata": {},
   "source": [
    "The reimplementation of the forward method is needed since we need to store the ys for giving to the head the right tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0696b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, core_model, output_idxs):\n",
    "        super().__init__()\n",
    "        self.core = core_model\n",
    "        self.idxs = output_idxs\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        x = self.core(*args, **kwargs)\n",
    "        return tuple(x if j == -1 else self.core.last_y[j] for j in self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_layers = list(core_model.model.children())\n",
    "last_layer = list_of_layers.pop(-1)\n",
    "\n",
    "core_model.model = torch.nn.Sequential(*list_of_layers)\n",
    "core_wrapper = CoreModelWrapper(core_model, last_layer.f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbc38",
   "metadata": {},
   "source": [
    "Now we are ready for optimizing the body of YOLOv5 using the `nebullvm` function `optimize_torch_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc4d01",
   "metadata": {},
   "source": [
    "Nebullvm was built to be very easy to use. To optimize a model, you only need to specify the model, the batch size and input size for each input tensor, and a directory in which to save the optimized model. In the example, we chose the same directory in which this notebook runs.\n",
    "\n",
    "With the latest API, there are two ways to use nebullvm:\n",
    "\n",
    "- Option A: Accelerate the model up to ~10 times without losing in performances (accuracy/precision/etc.)\n",
    "- Option B: Accelerate the model up to ~30 times with a pre-defined maximum loss in performances\n",
    "    - B1: Performance is measured on your dataset with a performance metric that you specify.\n",
    "    - B2: Performance is measured on your dataset using precision as a metric.\n",
    "    - B3: Performance is measured using precision as a metric.\n",
    "    \n",
    "To learn more about how to use nebullvm, check out the <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> readme on GitHub </a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb07403",
   "metadata": {},
   "source": [
    "In this example, we provide the code to run option B.2 model and you find commented out the code for option A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10deb914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Option A: 2-10x acceleration, NO performance loss\n",
    "# model_optimized = optimize_torch_model(\n",
    "#     model=core_wrapper,\n",
    "#     batch_size=1,\n",
    "#     input_sizes=[(3, 384, 640)],\n",
    "#     save_dir=\".\",\n",
    "#     use_torch_api=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c15b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"zidane.png\"\n",
    "Image.open(requests.get(imgs[0], stream=True).raw).save(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_crop(im, original_model, img_size):\n",
    "    p  =  next(original_model.parameters())\n",
    "    im = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im)\n",
    "    max_y, max_x = im.size\n",
    "    ptr_x = np.random.choice(max_x-img_size[0])\n",
    "    ptr_y = np.random.choice(max_y-img_size[1])\n",
    "    im = np.array(im.crop((ptr_y, ptr_x, ptr_y + img_size[1], ptr_x + img_size[0])))\n",
    "    x = np.expand_dims(im, axis=0)\n",
    "    x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # stack and BHWC to BCHW\n",
    "    x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51757959",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((read_and_crop(img_name, core_model, (384, 640)),), None) for _ in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01adfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B.2: 2-30x acceleration, maximum performance loss below your \n",
    "# perf_loss_ths, where performance is measured on your dataset dl_nebullvm\n",
    "# using precision as a metric. Read more on GitHub on how to set perf_loss_ths.\n",
    "\n",
    "model_optimized = optimize_torch_model(\n",
    "    model=core_wrapper,\n",
    "    save_dir=\".\",\n",
    "    dataloader=input_data,\n",
    "    use_torch_api=True,\n",
    "    perf_loss_ths=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2866c",
   "metadata": {},
   "source": [
    "Now let's regroup together the optimized body and the head of YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedYolo(torch.nn.Module):\n",
    "    def __init__(self, optimized_core, head_layer):\n",
    "        super().__init__()\n",
    "        self.core = optimized_core\n",
    "        self.head = head_layer\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = list(self.core(x)) # it's a tuple\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f973f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_core = OptimizedYolo(model_optimized, last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e39d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.model = final_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe285d08",
   "metadata": {},
   "source": [
    "Finally we can check the speedup. Let's calculate the time required to run a prediction as an average over 100 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for _ in range(100):\n",
    "    st = time.time()\n",
    "    results = model(imgs)\n",
    "    times.append((time.time() - st)*1000)\n",
    "yolo_optimized_time = sum(times) / len(times)\n",
    "print(f\"Average prediction time: {yolo_optimized_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50807de",
   "metadata": {},
   "source": [
    "What an amazing result, right?!? Stay tuned for more cool content from the Nebuly team :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012656ae",
   "metadata": {},
   "source": [
    "## Print the optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d65b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter here your username\n",
    "your_username = \"username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4433448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following line for installing gputil if you are running on an NVIDIA GPU.\n",
    "#!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30260a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpuinfo\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cpu_info = cpuinfo.get_cpu_info()['brand_raw']\n",
    "gpu_info = \"no\"\n",
    "if torch.cuda.is_available():\n",
    "    import GPUtil\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_info = list(gpus)[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Hello, I'm {your_username}!\n",
    "I've tested nebullvm on the following setup:\n",
    "\n",
    "Hardware: {cpu_info} CPU and {gpu_info} GPU.\n",
    "Model: YOLOv5s\n",
    "Vanilla performance: {round(yolo_vanilla_time, 2)}ms\n",
    "Optimized performance: {round(yolo_optimized_time, 2)}ms\n",
    "Speedup: {round(yolo_vanilla_time/yolo_optimized_time, 1)}x\n",
    "\"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c4485",
   "metadata": {},
   "source": [
    "Great! Was it easy? How are the results? Do you have any comments? Share your optimization results and thoughts with our community on Discord, where we chat about nebullvm and AI acceleration.\n",
    "\n",
    "Note that the acceleration of nebullvm depends very much on the hardware configuration and your AI model. Given the same input model, nebullvm can accelerate it by 10 times on some machines and perform poorly on others.\n",
    "\n",
    "If you want to learn more about how nebullvm works, look at other tutorials and performance benchmarks, check out the links below or write to us on Discord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28f13a",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#how-nebullvm-works\" target=\"_blank\" style=\"text-decoration: none;\"> How Nebullvm Works </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#tutorials\" target=\"_blank\" style=\"text-decoration: none;\"> Tutorials </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#benchmarks\" target=\"_blank\" style=\"text-decoration: none;\"> Benchmarks </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#installation\" target=\"_blank\" style=\"text-decoration: none;\"> Installation </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#get-started\" target=\"_blank\" style=\"text-decoration: none;\"> Get Started </a> â€¢\n",
    "    <a href=\"https://github.com/nebuly-ai/nebullvm#optimization-examples\" target=\"_blank\" style=\"text-decoration: none;\"> Optimization Examples </a>\n",
    "</center>\n",
    "<center> \n",
    "    <a href=\"https://discord.com/invite/RbeQMu886J\" target=\"_blank\" style=\"text-decoration: none;\"> Discord </a> |\n",
    "    <a href=\"https://nebuly.ai/\" target=\"_blank\" style=\"text-decoration: none;\"> Website </a> |\n",
    "    <a href=\"https://www.linkedin.com/company/72460022/\" target=\"_blank\" style=\"text-decoration: none;\"> LinkedIn </a> |\n",
    "    <a href=\"https://twitter.com/nebuly_ai\" target=\"_blank\" style=\"text-decoration: none;\"> Twitter </a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
